# src/llava_adapter/llava_wrapper.py
# -*- coding: utf-8 -*-

# ============================================================
# ğŸ”§ Gated steering é—¨æ§æ¨¡å¼å¼€å…³ï¼ˆåªå½±å“ GatedSteeredBlockï¼‰
# ä½ åªéœ€è¦æ”¹è¿™é‡Œï¼šTrue/False
# ============================================================
# Falseï¼šå®Œå…¨ä¿æŒåŸé€»è¾‘ï¼ˆuse_theta_tau=True -> sigmoid((s-theta)/tau)ï¼Œå¦åˆ™ sigmoid(s)ï¼‰
# True ï¼šå¼ºåˆ¶ä½¿ç”¨ sigmoid(s) ä½œä¸ºé—¨æ§æ¦‚ç‡ pï¼ˆå¿½ç•¥ theta/tauï¼Œå³ä½¿ use_theta_tau=Trueï¼‰
GATED_STEERING_USE_PLAIN_SIGMOID = True


# ============================================================
# ğŸ”§ Gated steering è°ƒè¯•å¼€å…³ï¼ˆåªå½±å“ GatedSteeredBlockï¼‰
# ä½ åªéœ€è¦æ”¹è¿™é‡Œï¼šTrue/False
# ============================================================
GATED_STEERING_DEBUG = False                 # æ€»å¼€å…³ï¼šTrue=æ‰“å°, False=å®Œå…¨ä¸æ‰“å°ï¼ˆè·Ÿä¹‹å‰ä¸€æ ·ï¼‰
GATED_STEERING_DEBUG_EVERY_N = 5            # æ¯éš” N æ­¥æ‰“å°ä¸€æ¬¡ï¼ˆé¿å…åˆ·å±ï¼‰
GATED_STEERING_DEBUG_MAX_STEPS = 20         # æ¯ä¸ªå±‚æœ€å¤šæ‰“å°å¤šå°‘æ­¥
GATED_STEERING_DEBUG_LAYERS = None          # None=æ‰€æœ‰æ³¨å…¥å±‚éƒ½æ‰“å°ï¼›ä¾‹å¦‚ {7, 15, 23}
GATED_STEERING_DEBUG_PRINT_THETA_TAU = True # æ˜¯å¦é¢å¤–æ‰“å° theta/tauï¼ˆuse_theta_tau=True ä¸”æœªå¼ºåˆ¶ plain sigmoid æ—¶ï¼‰


import os
import sys
from typing import List, Dict, Any, Optional, Callable

import torch
from torch import nn
from transformers import set_seed
import numpy as np


# ========= 1. LLaVA ä»“åº“è·¯å¾„ =========
DEFAULT_LLAVA_REPO = "/data/ruipeng.zhang/LLaVA"
LLAVA_REPO = os.environ.get("LLAVA_REPO", DEFAULT_LLAVA_REPO)
if LLAVA_REPO not in sys.path:
    sys.path.append(LLAVA_REPO)
sys.path.append("/data/ruipeng.zhang/LLaVA")

# ========= 2. å¼•å…¥ LLaVA ä¾èµ– =========
try:
    from llava.model.builder import load_pretrained_model
    from llava.mm_utils import (
        tokenizer_image_token,
        get_model_name_from_path,
        KeywordsStoppingCriteria,
    )
    from llava.conversation import conv_templates, SeparatorStyle
    from llava.constants import (
        IMAGE_TOKEN_INDEX,
        DEFAULT_IMAGE_TOKEN,
        DEFAULT_IM_START_TOKEN,
        DEFAULT_IM_END_TOKEN,
    )
    from llava.utils import disable_torch_init
except ImportError as e:
    load_pretrained_model = None
    raise ImportError(
        f"å¯¼å…¥ LLaVA ç›¸å…³æ¨¡å—å¤±è´¥ï¼Œè¯·æ£€æŸ¥ LLAVA_REPO è·¯å¾„æ˜¯å¦æ­£ç¡®: {LLAVA_REPO}\nåŸå§‹é”™è¯¯: {e}"
    )


# ========= 3. utils =========

def _to_str_local(x) -> str:
    """å…¼å®¹ numpy çš„ bytes <-> strã€‚"""
    if isinstance(x, str):
        return x
    if isinstance(x, (bytes, np.bytes_)):
        return x.decode("utf-8")
    try:
        return str(x)
    except Exception:
        return ""


def _normalize_vec(v: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
    n = v.norm(p=2)
    if n.item() <= eps:
        return v
    return v / n


def _gated_dbg_should_print(layer_id: int, step: int) -> bool:
    """Debug å…³é—­æ—¶å¿…é¡»æè½»é‡ï¼Œé¿å…ä»»ä½•åŒæ­¥/é¢å¤–å¼€é”€ã€‚"""
    if not GATED_STEERING_DEBUG:
        return False
    if GATED_STEERING_DEBUG_LAYERS is not None and layer_id not in GATED_STEERING_DEBUG_LAYERS:
        return False
    if step >= GATED_STEERING_DEBUG_MAX_STEPS:
        return False
    if GATED_STEERING_DEBUG_EVERY_N <= 1:
        return True
    return (step % GATED_STEERING_DEBUG_EVERY_N) == 0


# ========= 4. probe loaders =========

def load_probes_and_build_dirs_local(
    probe_path: str,
    steer_layers: List[int],
    normalize: bool = True,
    direction: str = "more_visual",   # "more_visual" æˆ– "less_visual"
) -> Dict[int, torch.Tensor]:
    """
    ä» binary_probes_by_range.npz é‡Œè¯»æ¯å±‚çš„ w_lï¼Œæ„é€  steering æ–¹å‘å‘é‡ã€‚
    è¿”å›:
        layer_id -> direction_l (torch.FloatTensor, shape=[hidden_dim], CPU float32)
    """
    probe_path = os.path.expanduser(probe_path)
    data = np.load(probe_path)

    layer_names = [_to_str_local(x) for x in data["layer_names"]]
    W = data["W"]  # [num_layers, hidden_dim]
    name2idx = {name: i for i, name in enumerate(layer_names)}

    dirs: Dict[int, torch.Tensor] = {}
    sign = 1.0 if direction == "more_visual" else -1.0

    for lid in steer_layers:
        lname = f"layer_{lid}"
        if lname not in name2idx:
            raise ValueError(f"probe æ–‡ä»¶é‡Œæ²¡æœ‰ {lname}ï¼Œå¯ç”¨å±‚å: {layer_names}")
        row = name2idx[lname]
        w_np = W[row]                      # [hidden_dim]
        w = torch.from_numpy(w_np).float() # CPU float32

        if normalize:
            norm = w.norm(p=2).item()
            if norm > 0:
                w = w / norm

        w = sign * w
        dirs[lid] = w

    return dirs


def load_hallu_gate_probes_local(
    gate_probe_path: str,
    steer_layers: List[int],
) -> Dict[int, Dict[str, torch.Tensor]]:
    """
    è¯»å– hallu_gate_probes_v1.npzï¼Œè¿”å›æ¯å±‚ gate å‚æ•°ï¼ˆCPU float32ï¼‰ï¼š
      lid -> {"w": [d], "b": [], "theta": [], "tau": []}
    æ³¨æ„ï¼šnpz é‡Œ layer_names å¯èƒ½æ˜¯ object arrayï¼Œéœ€è¦ allow_pickle=Trueã€‚
    """
    gate_probe_path = os.path.expanduser(gate_probe_path)
    data = np.load(gate_probe_path, allow_pickle=True)

    layer_names = [_to_str_local(x) for x in data["layer_names"]]
    W = data["W"]          # [L, d]
    b = data["b"]          # [L]
    theta = data["theta"] if "theta" in data.files else np.zeros((W.shape[0],), dtype=np.float32)
    tau = data["tau"] if "tau" in data.files else np.ones((W.shape[0],), dtype=np.float32)

    name2idx = {name: i for i, name in enumerate(layer_names)}

    out: Dict[int, Dict[str, torch.Tensor]] = {}
    for lid in steer_layers:
        lname = f"layer_{lid}"
        if lname not in name2idx:
            raise ValueError(f"[gate-probe] æ–‡ä»¶é‡Œæ²¡æœ‰ {lname}ï¼Œå¯ç”¨å±‚å: {layer_names}")

        row = name2idx[lname]
        w = torch.from_numpy(W[row]).float()                 # [d]
        bb = torch.tensor(float(b[row]), dtype=torch.float32)
        th = torch.tensor(float(theta[row]), dtype=torch.float32)
        ta = torch.tensor(float(tau[row]), dtype=torch.float32)
        out[lid] = {"w": w, "b": bb, "theta": th, "tau": ta}

    return out


# ========= 5. blocks =========

class GatedSteeredBlock(nn.Module):
    """
    åœ¨æ¯å±‚ forward å†…ï¼ˆåªæ”¹ last tokenï¼‰ï¼š
      s = w^T h_last + b
      p = sigmoid((s - theta)/tau)    # é»˜è®¤ï¼ˆuse_theta_tau=Trueï¼‰
          or sigmoid(s)               # use_theta_tau=False
          or sigmoid(s)               # è‹¥å…¨å±€ GATED_STEERING_USE_PLAIN_SIGMOID=Trueï¼Œåˆ™å¼ºåˆ¶ä½¿ç”¨

    âœ… æ–°æ³¨å…¥ç³»æ•°ï¼ˆä¿è¯æœ€åŸºæœ¬æ³¨å…¥ï¼‰ï¼š
      alpha = lambda*1/2 + (p)*lambda*1/2
            = lambda * (0.5 + 0.5*p)
      h_last <- h_last + alpha * direction_vec
    """

    def __init__(
        self,
        base_block: nn.Module,
        direction_vec: torch.Tensor,     # [d]
        gate_w: torch.Tensor,            # [d]
        gate_b: torch.Tensor,            # scalar
        gate_theta: torch.Tensor,        # scalar
        gate_tau: torch.Tensor,          # scalar
        lambda_scale: float,
        enable_steering: bool = True,
        use_theta_tau: bool = True,
        min_tau: float = 1e-6,
        clone_hidden: bool = True,       # ä¿å®ˆèµ·è§é»˜è®¤ cloneï¼›æƒ³æé€Ÿå¯å…³
    ):
        super().__init__()
        self.base_block = base_block

        # ä¸ persistentï¼Œé¿å…å†™è¿› state_dict
        self.register_buffer("direction_vec", direction_vec, persistent=False)
        self.register_buffer("gate_w", gate_w, persistent=False)
        self.register_buffer("gate_b", gate_b, persistent=False)
        self.register_buffer("gate_theta", gate_theta, persistent=False)
        self.register_buffer("gate_tau", gate_tau, persistent=False)

        self.lambda_scale = float(lambda_scale)
        self.enable_steering = bool(enable_steering)
        self.use_theta_tau = bool(use_theta_tau)
        self.min_tau = float(min_tau)
        self.clone_hidden = bool(clone_hidden)

        # debug æ ‡è¯†ï¼šç”±æ³¨å…¥å‡½æ•°å†™å…¥ layer_id
        self.layer_id: int = -1
        self._dbg_step: int = 0

    def forward(self, *args, **kwargs):
        out = self.base_block(*args, **kwargs)

        if isinstance(out, tuple):
            hidden = out[0]
            rest = out[1:]
            is_tuple = True
        else:
            hidden = out
            rest = None
            is_tuple = False

        if (not self.enable_steering) or (hidden is None) or (hidden.dim() != 3):
            return out

        # hidden: [bs, seq_len, d]
        if self.clone_hidden:
            hidden = hidden.clone()

        h_last = hidden[:, -1, :]  # [bs, d]

        # buffers åœ¨æ³¨å…¥æ—¶å·²å¯¹é½ device/dtypeï¼šè¿™é‡Œä¸ .to(...)ï¼Œå‡å°‘å¼€é”€
        dvec = self.direction_vec                  # [d]
        w = self.gate_w                            # [d]
        b = self.gate_b                            # []
        s = (h_last * w).sum(dim=-1, keepdim=True) + b  # [bs,1]

        # âœ… é¢å¤–ç®—ä¸€ä»½ sigmoid(s)ï¼Œç”¨äºè§‚æµ‹/å¯¹æ¯”ï¼ˆä»¥åŠ plain æ¨¡å¼ä¸‹ç›´æ¥å¤ç”¨ï¼‰
        p_sig = torch.sigmoid(s)  # [bs,1]

        # âœ… é€‰æ‹©â€œæƒé‡ pâ€
        if (not GATED_STEERING_USE_PLAIN_SIGMOID) and self.use_theta_tau:
            theta = self.gate_theta
            tau = torch.clamp(self.gate_tau, min=self.min_tau)
            p = torch.sigmoid((s - theta) / tau)   # [bs,1]
            mode = "theta_tau"
        else:
            p = p_sig                               # [bs,1]
            mode = "plain_sigmoid"

        # âœ… æ–°çš„ alphaï¼šlambda*1/2 + p*lambda*1/2ï¼ˆç¡®ä¿æœ€åŸºæœ¬æ³¨å…¥ï¼‰
        lam = float(self.lambda_scale)
        alpha_base = 0.5 * self.lambda_scale                 # scalar (float)
        alpha_gate = 0.5 * self.lambda_scale * p             # [bs,1]
        alpha = alpha_base + alpha_gate                      # [bs,1]

        hidden[:, -1, :] = h_last + alpha * dvec             # [bs,d]

        # ---- debugï¼ˆåªåœ¨å¼€å…³æ‰“å¼€æ—¶æ‰§è¡Œï¼›å¦åˆ™å®Œå…¨ä¸åŒæ­¥/ä¸printï¼‰----
        if GATED_STEERING_DEBUG:
            step = self._dbg_step
            if _gated_dbg_should_print(self.layer_id, step):
                # åªæ‰“å° batch ç¬¬ 1 æ¡æ ·æœ¬çš„æ ‡é‡ï¼Œé¿å…åˆ·å±/å‡å°‘é¢å¤–ç»Ÿè®¡
                s0 = float(s[0, 0].detach().float().item())
                sig0 = float(p_sig[0, 0].detach().float().item())  # sigmoid(s0)
                p0 = float(p[0, 0].detach().float().item())        # å®é™…é—¨æ§æƒé‡
                abase0 = float(alpha_base)                         # lambda/2
                agate0 = float(alpha_gate[0, 0].detach().float().item())  # p*lambda/2
                a0 = float(alpha[0, 0].detach().float().item())    # æ€» alpha

                if (not GATED_STEERING_USE_PLAIN_SIGMOID) and self.use_theta_tau and GATED_STEERING_DEBUG_PRINT_THETA_TAU:
                    th0 = float(self.gate_theta.detach().float().item())
                    tau0 = float(torch.clamp(self.gate_tau, min=self.min_tau).detach().float().item())
                    print(
                        f"[gated][layer={self.layer_id}][step={step}][mode={mode}] "
                        f"s0={s0:.4f} sigmoid(s0)={sig0:.4f} p0={p0:.4f} "
                        f"alpha_base(lam/2)={abase0:.4f} alpha_gate(p*lam/2)={agate0:.4f} alpha0={a0:.4f} lam={lam:.4f} "
                        f"theta={th0:.4f} tau={tau0:.6f}"
                    )
                else:
                    print(
                        f"[gated][layer={self.layer_id}][step={step}][mode={mode}] "
                        f"s0={s0:.4f} sigmoid(s0)={sig0:.4f} p0={p0:.4f} "
                        f"alpha_base(lam/2)={abase0:.4f} alpha_gate(p*lam/2)={agate0:.4f} alpha0={a0:.4f} lam={lam:.4f}"
                    )

            self._dbg_step = step + 1

        if is_tuple:
            return (hidden, *rest)
        else:
            return hidden


class SteeredBlock(nn.Module):
    """ç®€å•ç‰ˆï¼šlast token åŠ å›ºå®šæ–¹å‘å‘é‡ã€‚"""

    def __init__(
        self,
        base_block: nn.Module,
        direction_vec: torch.Tensor,
        lambda_scale: float,
        enable_steering: bool = True,
        clone_hidden: bool = True,   # ä¿å®ˆèµ·è§é»˜è®¤ cloneï¼›æƒ³æé€Ÿå¯å…³
    ):
        super().__init__()
        self.base_block = base_block
        self.register_buffer("direction_vec", direction_vec, persistent=False)
        self.lambda_scale = float(lambda_scale)
        self.enable_steering = bool(enable_steering)
        self.clone_hidden = bool(clone_hidden)

    def forward(self, *args, **kwargs):
        out = self.base_block(*args, **kwargs)

        if isinstance(out, tuple):
            hidden = out[0]
            rest = out[1:]
            is_tuple = True
        else:
            hidden = out
            rest = None
            is_tuple = False

        if (not self.enable_steering) or (hidden is None) or (hidden.dim() != 3):
            return out

        if self.clone_hidden:
            hidden = hidden.clone()

        d = self.direction_vec  # æ³¨å…¥æ—¶å·²å¯¹é½ device/dtype
        hidden[:, -1, :] = hidden[:, -1, :] + self.lambda_scale * d

        if is_tuple:
            return (hidden, *rest)
        else:
            return hidden


def _unwrap_to_base_block(block: nn.Module) -> nn.Module:
    """
    é¿å…â€œå¥—å¨ƒâ€ï¼šåå¤å‰¥ç¦» SteeredBlock / GatedSteeredBlockï¼Œæ‹¿åˆ°æœ€åº•å±‚ base_blockã€‚
    """
    cur = block
    for _ in range(8):
        if isinstance(cur, SteeredBlock):
            cur = cur.base_block
            continue
        if isinstance(cur, GatedSteeredBlock):
            cur = cur.base_block
            continue
        break
    return cur


# ========= 6. main wrapper =========

class LlavaHookedModel(nn.Module):
    """
    - åŠ è½½ LLaVA æ¨¡å‹ & tokenizer & image_processor
    - æ”¯æŒ forward hookï¼ˆé‡‡ hiddenï¼‰
    - æ”¯æŒ SteeredBlock æ³¨å…¥ï¼ˆå›ºå®š steeringï¼‰
    - æ”¯æŒ GatedSteeredBlock æ³¨å…¥ï¼ˆhallu gate åŠ¨æ€ steeringï¼‰
    """

    def __init__(
        self,
        model_path: str,
        model_base: Optional[str] = None,
        conv_mode: str = "llava_v1",
        device: str = "cuda",
        dtype: torch.dtype = torch.float16,
        seed: int = 42,
        llava_extra_args: Optional[Dict[str, Any]] = None,
    ):
        super().__init__()

        if load_pretrained_model is None:
            raise RuntimeError("load_pretrained_model æœªæ­£ç¡®å¯¼å…¥ï¼Œè¯·æ£€æŸ¥ LLaVA è·¯å¾„ã€‚")

        disable_torch_init()
        set_seed(seed)

        self.device = device
        self.dtype = dtype
        self.conv_mode = conv_mode

        llava_extra_args = llava_extra_args or {}

        model_path = os.path.expanduser(model_path)
        model_name = get_model_name_from_path(model_path)

        print(f"[LlavaHookedModel] Loading LLaVA from: {model_path}")
        print(f"[LlavaHookedModel] Parsed model_name: {model_name}")

        tokenizer, model, image_processor, _ = load_pretrained_model(
            model_path=model_path,
            model_base=model_base,
            model_name=model_name,
            device=device,
            device_map=None,  # å…³é”®ï¼šé¿å… mm æ¨¡å—è¢«åˆ†é…åˆ°å¥‡æ€ªè®¾å¤‡
            **llava_extra_args,
        )

        model.to(device)
        model.eval()

        self.tokenizer = tokenizer
        self.model = model
        self.image_processor = image_processor

        # hook
        self._hook_handles: List[Any] = []
        self._hook_buffers: Dict[str, List[torch.Tensor]] = {}

        # fixed steering
        self._steering_layers: List[int] = []
        self._steering_injected: bool = False

        # gated steering
        self._gated_steering_layers: List[int] = []
        self._gated_steering_injected: bool = False

    # ========= hook =========

    def _make_hook(self, name: str) -> Callable:
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                last_token = output[:, -1, :].detach().to("cpu")
            else:
                last_token = output[0][:, -1, :].detach().to("cpu")

            if name not in self._hook_buffers:
                self._hook_buffers[name] = []
            self._hook_buffers[name].append(last_token)
        return hook

    def register_hidden_hooks(self, layer_indices: List[int]):
        self.clear_hooks()
        self._hook_buffers.clear()

        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            raise RuntimeError("æ— æ³•è®¿é—® self.model.model.layersï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")

        for idx in layer_indices:
            if idx < 0 or idx >= len(decoder_layers):
                raise ValueError(f"layer index {idx} è¶…å‡ºèŒƒå›´ [0, {len(decoder_layers) - 1}]")
            layer = decoder_layers[idx]
            handle = layer.register_forward_hook(self._make_hook(name=f"layer_{idx}"))
            self._hook_handles.append(handle)

    def clear_hooks(self):
        for h in self._hook_handles:
            h.remove()
        self._hook_handles = []

    def pop_hook_buffers(self) -> Dict[str, List[torch.Tensor]]:
        buffers = self._hook_buffers
        self._hook_buffers = {}
        return buffers

    # ========= fixed steering injection =========

    def inject_steering_blocks_from_probes(
        self,
        probe_path: str,
        steer_layers: List[int],
        lambda_scale: float = 1.0,
        normalize: bool = True,
        direction: str = "more_visual",
        clone_hidden: bool = True,
    ):
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            raise RuntimeError("æ— æ³•è®¿é—® self.model.model.layersï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")

        dirs = load_probes_and_build_dirs_local(
            probe_path=probe_path,
            steer_layers=steer_layers,
            normalize=normalize,
            direction=direction,
        )

        model_device = next(self.model.parameters()).device
        model_dtype = next(self.model.parameters()).dtype

        for lid in steer_layers:
            if lid < 0 or lid >= len(decoder_layers):
                raise ValueError(f"steer_layers ä¸­çš„å±‚å· {lid} è¶…å‡ºèŒƒå›´ [0, {len(decoder_layers)-1}]")

            cur = decoder_layers[lid]
            base_block = _unwrap_to_base_block(cur)

            dir_vec = dirs[lid].to(device=model_device, dtype=model_dtype)

            if isinstance(cur, SteeredBlock) and _unwrap_to_base_block(cur) is base_block:
                cur.base_block = base_block
                cur.direction_vec = dir_vec
                cur.lambda_scale = float(lambda_scale)
                cur.enable_steering = True
                cur.clone_hidden = bool(clone_hidden)
                print(f"[steering-block] update layer_{lid}, lambda={lambda_scale:.4f}")
            else:
                decoder_layers[lid] = SteeredBlock(
                    base_block=base_block,
                    direction_vec=dir_vec,
                    lambda_scale=lambda_scale,
                    enable_steering=True,
                    clone_hidden=clone_hidden,
                )
                print(f"[steering-block] replace layer_{lid}, lambda={lambda_scale:.4f}")

        self._steering_layers = list(steer_layers)
        self._steering_injected = True

    def enable_steering(self):
        if not self._steering_injected:
            return
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return
        for lid in self._steering_layers:
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], SteeredBlock):
                decoder_layers[lid].enable_steering = True
        print(f"[steering-block] enable: {self._steering_layers}")

    def disable_steering(self):
        if not self._steering_injected:
            return
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return
        for lid in self._steering_layers:
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], SteeredBlock):
                decoder_layers[lid].enable_steering = False
        print(f"[steering-block] disable: {self._steering_layers}")

    # ========= gated steering injection =========

    def inject_gated_steering_blocks_from_hallu_gate(
        self,
        gate_probe_path: str,
        steer_layers: List[int],
        lambda_scale: float = 1.0,
        use_theta_tau: bool = True,
        dir_from_gate: bool = True,
        dir_sign: float = -1.0,
        dir_normalize: bool = True,
        direction_probe_path: Optional[str] = None,
        direction_probe_normalize: bool = True,
        direction_probe_mode: str = "more_visual",
        clone_hidden: bool = True,
    ):
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            raise RuntimeError("æ— æ³•è®¿é—® self.model.model.layersï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„ã€‚")

        gate = load_hallu_gate_probes_local(gate_probe_path, steer_layers)

        dirs: Dict[int, torch.Tensor] = {}
        if direction_probe_path is not None:
            dirs = load_probes_and_build_dirs_local(
                probe_path=direction_probe_path,
                steer_layers=steer_layers,
                normalize=direction_probe_normalize,
                direction=direction_probe_mode,
            )
        else:
            if not dir_from_gate:
                raise ValueError("dir_from_gate=False ä¸”æœªæä¾› direction_probe_pathï¼Œæ— æ³•æ„é€  direction_vecã€‚")
            for lid in steer_layers:
                ww = gate[lid]["w"].clone()
                if dir_normalize:
                    ww = _normalize_vec(ww)
                dirs[lid] = float(dir_sign) * ww

        model_device = next(self.model.parameters()).device
        model_dtype = next(self.model.parameters()).dtype

        for lid in steer_layers:
            if lid < 0 or lid >= len(decoder_layers):
                raise ValueError(f"layer {lid} out of range [0,{len(decoder_layers)-1}]")

            cur = decoder_layers[lid]
            base_block = _unwrap_to_base_block(cur)

            dir_vec = dirs[lid].to(device=model_device, dtype=model_dtype)

            gw = gate[lid]["w"].to(device=model_device, dtype=model_dtype)
            gb = gate[lid]["b"].to(device=model_device, dtype=model_dtype)
            gth = gate[lid]["theta"].to(device=model_device, dtype=model_dtype)
            gta = gate[lid]["tau"].to(device=model_device, dtype=model_dtype)

            if isinstance(cur, GatedSteeredBlock) and _unwrap_to_base_block(cur) is base_block:
                cur.base_block = base_block
                cur.direction_vec = dir_vec
                cur.gate_w = gw
                cur.gate_b = gb
                cur.gate_theta = gth
                cur.gate_tau = gta
                cur.lambda_scale = float(lambda_scale)
                cur.use_theta_tau = bool(use_theta_tau)
                cur.enable_steering = True
                cur.clone_hidden = bool(clone_hidden)
                cur.layer_id = int(lid)     # âœ… ç»™ debug ç”¨
                cur._dbg_step = 0           # âœ… æ¯æ¬¡æ³¨å…¥é‡ç½®è®¡æ•°ï¼ˆæ›´ç›´è§‚ï¼‰
                print(f"[gated-steering] update layer_{lid}, lambda={lambda_scale:.4f}")
            else:
                blk = GatedSteeredBlock(
                    base_block=base_block,
                    direction_vec=dir_vec,
                    gate_w=gw,
                    gate_b=gb,
                    gate_theta=gth,
                    gate_tau=gta,
                    lambda_scale=lambda_scale,
                    enable_steering=True,
                    use_theta_tau=use_theta_tau,
                    clone_hidden=clone_hidden,
                )
                blk.layer_id = int(lid)     # âœ… ç»™ debug ç”¨
                blk._dbg_step = 0
                decoder_layers[lid] = blk
                print(f"[gated-steering] replace layer_{lid}, lambda={lambda_scale:.4f}")

        self._gated_steering_layers = list(steer_layers)
        self._gated_steering_injected = True

    def enable_gated_steering(self):
        if not self._gated_steering_injected:
            return
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return
        for lid in self._gated_steering_layers:
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], GatedSteeredBlock):
                decoder_layers[lid].enable_steering = True
        print(f"[gated-steering] enable: {self._gated_steering_layers}")

    def disable_gated_steering(self):
        if not self._gated_steering_injected:
            return
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return
        for lid in self._gated_steering_layers:
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], GatedSteeredBlock):
                decoder_layers[lid].enable_steering = False
        print(f"[gated-steering] disable: {self._gated_steering_layers}")

    @torch.no_grad()
    def generate_gated(
        self,
        image,
        query_text: str,
        max_new_tokens: int = 64,
        temperature: float = 0.0,
        num_beams: int = 1,
        use_image: bool = True,
        gate_probe_path: str = "/nas_data/ruipeng.zhang/rlhfv_hallu_hidden_llava/hallu_gate_probes_v1.npz",
        steer_layers: Optional[List[int]] = None,
        lambda_scale: float = 1.0,
        use_theta_tau: bool = True,
        dir_sign: float = -1.0,
        dir_normalize: bool = True,
        direction_probe_path: Optional[str] = None,
        direction_probe_normalize: bool = True,
        direction_probe_mode: str = "more_visual",
        auto_disable: bool = True,
        clone_hidden: bool = True,
        **gen_kwargs,
    ) -> Dict[str, Any]:
        if steer_layers is None:
            steer_layers = list(range(0, 32))

        self.inject_gated_steering_blocks_from_hallu_gate(
            gate_probe_path=gate_probe_path,
            steer_layers=steer_layers,
            lambda_scale=lambda_scale,
            use_theta_tau=use_theta_tau,
            dir_from_gate=True,
            dir_sign=dir_sign,
            dir_normalize=dir_normalize,
            direction_probe_path=direction_probe_path,
            direction_probe_normalize=direction_probe_normalize,
            direction_probe_mode=direction_probe_mode,
            clone_hidden=clone_hidden,
        )

        self.enable_gated_steering()

        out = self.generate(
            image=image,
            query_text=query_text,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            num_beams=num_beams,
            use_image=use_image,
            **gen_kwargs,
        )

        if auto_disable:
            self.disable_gated_steering()

        return out

    # ========= prompt/input building =========

    def _build_inputs(self, image, query_text: str, with_image: bool = True):
        device = self.device

        if with_image:
            if getattr(self.model.config, "mm_use_im_start_end", False):
                qs = (
                    DEFAULT_IM_START_TOKEN
                    + DEFAULT_IMAGE_TOKEN
                    + DEFAULT_IM_END_TOKEN
                    + "\n"
                    + query_text
                )
            else:
                qs = DEFAULT_IMAGE_TOKEN + "\n" + query_text

            conv = conv_templates[self.conv_mode].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt = conv.get_prompt()

            input_ids = tokenizer_image_token(
                prompt,
                self.tokenizer,
                IMAGE_TOKEN_INDEX,
                return_tensors="pt",
            ).unsqueeze(0).to(device)

            if image is not None:
                image_tensor = self.image_processor.preprocess(
                    image,
                    return_tensors="pt",
                )["pixel_values"].to(device=device, dtype=self.model.dtype)
            else:
                image_tensor = None
        else:
            qs = query_text
            conv = conv_templates[self.conv_mode].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt = conv.get_prompt()

            input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(device)
            image_tensor = None

        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
        keywords = [stop_str]
        stopping_criteria = [KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)]
        return input_ids, image_tensor, stop_str, stopping_criteria

    def _safe_decode_ids(self, ids, skip_special_tokens: bool = False) -> str:
        if isinstance(ids, torch.Tensor):
            ids = ids.tolist()
        vocab_size = self.tokenizer.vocab_size
        safe_ids = [int(t) for t in ids if 0 <= int(t) < vocab_size]
        return self.tokenizer.decode(safe_ids, skip_special_tokens=skip_special_tokens)

    # ========= generate =========

    @torch.no_grad()
    def generate(
        self,
        image,
        query_text: str,
        max_new_tokens: int = 64,
        temperature: float = 0.0,
        num_beams: int = 1,
        use_image: bool = True,
        **gen_kwargs,
    ) -> Dict[str, Any]:
        input_ids, image_tensor, stop_str, stopping_criteria = self._build_inputs(
            image=image,
            with_image=use_image,
            query_text=query_text,
        )

        do_sample = temperature > 0.0
        gen_outputs = self.model.generate(
            input_ids,
            images=image_tensor,
            do_sample=do_sample,
            num_beams=num_beams,
            max_new_tokens=max_new_tokens,
            use_cache=True,
            stopping_criteria=stopping_criteria,
            **gen_kwargs,
        )

        output_ids = gen_outputs.sequences if hasattr(gen_outputs, "sequences") else gen_outputs

        seq = output_ids[0]
        prompt = input_ids[0]

        if seq.shape[0] >= prompt.shape[0] and torch.equal(seq[: prompt.shape[0]], prompt):
            gen_token_ids = seq[prompt.shape[0]:].unsqueeze(0)
        else:
            gen_token_ids = seq.unsqueeze(0)

        gen_token_ids_cpu = gen_token_ids[0].detach().to("cpu")
        outputs = self._safe_decode_ids(gen_token_ids_cpu, skip_special_tokens=True).strip()

        if outputs.endswith(stop_str):
            outputs = outputs[: -len(stop_str)].strip()

        hook_buffers = self.pop_hook_buffers()

        return {
            "output_text": outputs,
            "hook_buffers": hook_buffers,
            "output_ids": gen_token_ids_cpu,
        }

    # ========= probe forward (teacher forcing) =========

    def _build_qa_inputs_for_probe(
        self,
        image,
        query_text: str,
        answer_text: str,
        with_image: bool = True,
    ):
        device = self.device

        if with_image:
            if getattr(self.model.config, "mm_use_im_start_end", False):
                qs = (
                    DEFAULT_IM_START_TOKEN
                    + DEFAULT_IMAGE_TOKEN
                    + DEFAULT_IM_END_TOKEN
                    + "\n"
                    + query_text
                )
            else:
                qs = DEFAULT_IMAGE_TOKEN + "\n" + query_text
        else:
            qs = query_text

        base_conv = conv_templates[self.conv_mode].copy()
        base_conv.append_message(base_conv.roles[0], qs)

        conv_prompt = base_conv.copy()
        conv_prompt.append_message(conv_prompt.roles[1], None)
        prompt_only = conv_prompt.get_prompt()

        conv_full = base_conv.copy()
        conv_full.append_message(conv_full.roles[1], answer_text)
        prompt_full = conv_full.get_prompt()

        if with_image:
            input_ids_prompt = tokenizer_image_token(
                prompt_only, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt"
            ).unsqueeze(0).to(device)

            input_ids_full = tokenizer_image_token(
                prompt_full, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt"
            ).unsqueeze(0).to(device)

            image_tensor = None
            if image is not None:
                image_tensor = self.image_processor.preprocess(
                    image, return_tensors="pt"
                )["pixel_values"].to(device=device, dtype=self.model.dtype)
        else:
            input_ids_prompt = self.tokenizer(prompt_only, return_tensors="pt").input_ids.to(device)
            input_ids_full = self.tokenizer(prompt_full, return_tensors="pt").input_ids.to(device)
            image_tensor = None

        prompt_len = int(input_ids_prompt.shape[1])
        return input_ids_full, image_tensor, prompt_len

    @torch.no_grad()
    def forward_for_probe(
        self,
        image,
        query_text: str,
        answer_text: str,
        use_image: bool = True,
    ) -> Dict[str, Any]:
        input_ids_full, image_tensor, prompt_len = self._build_qa_inputs_for_probe(
            image=image,
            query_text=query_text,
            answer_text=answer_text,
            with_image=use_image,
        )

        outputs = self.model(
            input_ids_full,
            images=image_tensor,
            output_hidden_states=True,
            use_cache=False,
        )

        logits = outputs.logits[0].detach().to("cpu")  # [T, V]
        hidden_states = outputs.hidden_states          # len = L+1 (emb + layers)

        hidden_dict: Dict[str, torch.Tensor] = {}
        for layer_idx, h in enumerate(hidden_states[1:]):
            hidden_dict[f"layer_{layer_idx}"] = h[0].detach().to("cpu")  # [T, d]

        return {
            "input_ids": input_ids_full[0].detach().to("cpu"),
            "logits": logits,
            "hidden_states": hidden_dict,
            "prompt_len": int(prompt_len),
        }
    # ========= (NEW) silent steering toggles (no print) =========

    def _silent_set_fixed_steering(self, enabled: bool):
        """é™é»˜å¼€å…³ fixed steeringï¼ˆä¸ printï¼Œä¸æ”¹å˜å…¶å®ƒé€»è¾‘ï¼‰"""
        if not self._steering_injected:
            return
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return
        for lid in self._steering_layers:
            if 0 <= lid < len(decoder_layers):
                blk = decoder_layers[lid]
                if isinstance(blk, SteeredBlock):
                    blk.enable_steering = bool(enabled)

    def _silent_set_gated_steering(self, enabled: bool):
        """é™é»˜å¼€å…³ gated steeringï¼ˆä¸ printï¼‰"""
        if not self._gated_steering_injected:
            return
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return
        for lid in self._gated_steering_layers:
            if 0 <= lid < len(decoder_layers):
                blk = decoder_layers[lid]
                if isinstance(blk, GatedSteeredBlock):
                    blk.enable_steering = bool(enabled)

    def _snapshot_steering_state(self):
        """ä¿å­˜å½“å‰ steering å¼€å…³çŠ¶æ€ï¼Œä¾¿äº TF ç»“æŸåæ¢å¤ï¼Œé¿å…å½±å“å¤–éƒ¨è„šæœ¬ã€‚"""
        st = {"fixed": {}, "gated": {}}
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return st

        for lid in self._steering_layers:
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], SteeredBlock):
                st["fixed"][lid] = bool(decoder_layers[lid].enable_steering)

        for lid in self._gated_steering_layers:
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], GatedSteeredBlock):
                st["gated"][lid] = bool(decoder_layers[lid].enable_steering)

        return st

    def _restore_steering_state(self, st):
        """æ¢å¤ steering çŠ¶æ€"""
        try:
            decoder_layers = self.model.model.layers
        except AttributeError:
            return

        for lid, v in (st.get("fixed", {}) or {}).items():
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], SteeredBlock):
                decoder_layers[lid].enable_steering = bool(v)

        for lid, v in (st.get("gated", {}) or {}).items():
            if 0 <= lid < len(decoder_layers) and isinstance(decoder_layers[lid], GatedSteeredBlock):
                decoder_layers[lid].enable_steering = bool(v)

    # ========= (NEW) stepwise teacher forcing for token-level diagnostics =========

    @torch.no_grad()
    def forward_for_probe_stepwise(
        self,
        image,
        query_text: str,
        answer_text: str,
        use_image: bool = True,
        steering_mode: str = "none",  # "none" | "global" | "oracle"
        oracle_mask: Optional[List[bool]] = None,  # len == answer_len
        steer_kind: str = "fixed",  # "fixed" | "gated" | "both"
        compute_entropy: bool = True,
    ) -> Dict[str, Any]:
        """
        âœ… é€ token çš„ teacher forcingï¼ˆå¸¦ KV cacheï¼‰ï¼Œç¡®ä¿ä½ çš„ SteeredBlock/GatedSteeredBlock çš„ â€œlast token æ³¨å…¥â€
        åœ¨æ¯ä¸ª step éƒ½ç”Ÿæ•ˆï¼Œä»è€Œæ”¯æŒ try-oracle-gatingã€‚

        è¿”å›ï¼š
          - answer_ids: [A] CPU
          - logprobs:   List[float] é•¿åº¦ Aï¼Œæ¯ä¸ª token çš„ log p(y_t | prefix)
          - entropies:  List[float] é•¿åº¦ Aï¼ˆå¯é€‰ï¼‰
          - prompt_len: int
        """

        if steering_mode not in ("none", "global", "oracle"):
            raise ValueError(f"steering_mode must be none/global/oracle, got {steering_mode}")
        if steer_kind not in ("fixed", "gated", "both"):
            raise ValueError(f"steer_kind must be fixed/gated/both, got {steer_kind}")

        # build ids & image tensor (ä¸åŸé€»è¾‘ä¸€è‡´)
        input_ids_full, image_tensor, prompt_len = self._build_qa_inputs_for_probe(
            image=image,
            query_text=query_text,
            answer_text=answer_text,
            with_image=use_image,
        )

        prompt_ids = input_ids_full[:, :prompt_len]       # [1, P]
        answer_ids = input_ids_full[:, prompt_len:]       # [1, A]
        A = int(answer_ids.shape[1])

        if steering_mode == "oracle":
            if oracle_mask is None:
                raise ValueError("steering_mode=oracle requires oracle_mask")
            if len(oracle_mask) != A:
                raise ValueError(f"oracle_mask length {len(oracle_mask)} != answer_len {A}")

        # ä¿å­˜ & æ¢å¤çŠ¶æ€ï¼Œç¡®ä¿ä¸å½±å“å¤–éƒ¨è„šæœ¬
        st0 = self._snapshot_steering_state()

        def _set_enabled(enabled: bool):
            if steer_kind in ("fixed", "both"):
                self._silent_set_fixed_steering(enabled)
            if steer_kind in ("gated", "both"):
                self._silent_set_gated_steering(enabled)

        logprobs: List[float] = []
        entropies: List[float] = []

        past = None
        cur_input = prompt_ids  # ç¬¬ä¸€æ­¥ç”¨ prompt é¢„å¡«å……ï¼Œé¢„æµ‹ answer ç¬¬ä¸€ä¸ª token

        try:
            for t in range(A):
                # --- è®¾ç½®æœ¬ step æ˜¯å¦æ³¨å…¥ ---
                if steering_mode == "none":
                    _set_enabled(False)
                elif steering_mode == "global":
                    _set_enabled(True)
                else:  # oracle
                    _set_enabled(bool(oracle_mask[t]))

                outputs = self.model(
                    cur_input,
                    images=image_tensor,
                    use_cache=True,
                    past_key_values=past,
                )
                logits_last = outputs.logits[:, -1, :]  # [1, V]
                past = outputs.past_key_values

                # å½“å‰ç›®æ ‡ token
                tgt = answer_ids[:, t]  # [1]

                # logprob
                logp = torch.log_softmax(logits_last, dim=-1)[0, int(tgt.item())].item()
                logprobs.append(float(logp))

                # entropyï¼ˆå¯é€‰ï¼‰
                if compute_entropy:
                    p = torch.softmax(logits_last, dim=-1)
                    H = (-(p * torch.log(p + 1e-12)).sum(dim=-1)[0]).item()
                    entropies.append(float(H))

                # ä¸‹ä¸€æ­¥è¾“å…¥ï¼šå–‚å…¥å½“å‰ tokenï¼ˆå½¢çŠ¶ [1,1]ï¼‰
                cur_input = tgt.view(1, 1)

        finally:
            # æ¢å¤ steering çŠ¶æ€
            self._restore_steering_state(st0)

        return {
            "answer_ids": answer_ids[0].detach().to("cpu"),
            "logprobs": logprobs,
            "entropies": entropies if compute_entropy else None,
            "prompt_len": int(prompt_len),
        }
